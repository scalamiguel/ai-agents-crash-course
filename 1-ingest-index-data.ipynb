{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d300c15f",
   "metadata": {},
   "source": [
    "# Day 1: Ingest and Index Your Data\n",
    "\n",
    "Welcome to our crash course\\!\n",
    "\n",
    "In this course, you'll learn how to build intelligent systems that can understand and interact with your data.\n",
    "\n",
    "We'll create a conversational agent that can answer questions about any GitHub repository \\- think of it as your personal AI assistant for documentation and code. If you know [DeepWiki](https://deepwiki.org/), it's something similar, but tailored to your GitHub repo.\n",
    "\n",
    "For that, we need to:\n",
    "\n",
    "- Download and process data from the repo  \n",
    "- Put it inside a search engine  \n",
    "- Make the search engine available to our agent\n",
    "\n",
    "In the first half of the course, we will focus on data preparation. Today, we will do the first part: downloading the data.\n",
    "\n",
    "# GitHub Repo Data\n",
    "\n",
    "On the first day, we will learn how to download and process data from any GitHub repository. We will download the data as a zip archive, process all the text data from there, and make it available for ingesting it later into a search engine.\n",
    "\n",
    "Think of this as preparing a meal \\- we need to gather and prep all our ingredients (the data) before we can cook (build our AI agent).\n",
    "\n",
    "Today, we will deal with simple cases, when documents are not large.\n",
    "\n",
    "Tomorrow we will deal with more complex cases when documents are big and we also have code.\n",
    "\n",
    "# Environment Setup\n",
    "\n",
    "First, let's prepare the environment. We need Python 3.10 or higher.\n",
    "\n",
    "We will use `uv` as the package manager. If you don't have `uv`, let's install it:\n",
    "\n",
    "```shell\n",
    "pip install uv\n",
    "```\n",
    "\n",
    "Next, create a folder `aihero` with two subfolders:\n",
    "\n",
    "- `course` \\- here you will reproduce all the examples from this email course  \n",
    "- `project` \\- here you will create your own project\n",
    "\n",
    "Now go to `course` and run:\n",
    "\n",
    "```shell\n",
    "uv init\n",
    "uv add requests python-frontmatter\n",
    "uv add --dev jupyter\n",
    "```\n",
    "\n",
    "This will initialize an empty Python project with `uv` and install multiple libraries:\n",
    "\n",
    "- `requests` for downloading data from GitHub  \n",
    "- `python-frontmatter` for parsing structured metadata in markdown files  \n",
    "- `jupyter` (in dev mode)\n",
    "\n",
    "The reason we need jupyter in dev mode is because it's only used for development and experimentation, not in the final production code.\n",
    "\n",
    "Let's start Jupyter:\n",
    "\n",
    "```shell\n",
    "uv run jupyter notebook\n",
    "```\n",
    "\n",
    "# Understanding Frontmatter\n",
    "\n",
    "We will also need a library for parsing frontmatter \\- a popular documentation format commonly used for modern frameworks like Jekyll, Hugo, and Next.js.\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "```\n",
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n",
    "```\n",
    "\n",
    "This format is called \"frontmatter\". The section between the `---` markers contains YAML metadata that describes the document, while everything below is regular Markdown content. This is very useful because we can extract structured information (like title, tags, difficulty level) along with the content.\n",
    "\n",
    "This is how we read it:\n",
    "\n",
    "```py\n",
    "import frontmatter\n",
    "\n",
    "with open('example.md', 'r', encoding='utf-8') as f:\n",
    "    post = frontmatter.load(f)\n",
    "\n",
    "# Access metadata\n",
    "print(post.metadata['title'])  # \"Getting Started with AI\"\n",
    "print(post.metadata['tags'])   # [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "\n",
    "# Access content\n",
    "print(post.content)  # The markdown content without frontmatter\n",
    "```\n",
    "\n",
    "We can also get all the metadata and content at the same time using the `post.to_dict()` method.\n",
    "\n",
    "# Sample Repositories\n",
    "\n",
    "Now that we know how to process a single markdown file, let's find a repo with multiple files that we will use as our knowledge base.\n",
    "\n",
    "We will work with multiple repositories:\n",
    "\n",
    "- [https://github.com/DataTalksClub/faq](https://github.com/DataTalksClub/faq) (source for [https://datatalks.club/faq/](https://datatalks.club/faq/)) \\- FAQ for DataTalks.Club courses  \n",
    "- [https://github.com/evidentlyai/docs/](https://github.com/evidentlyai/docs/) \\- docs for Evidently AI library\n",
    "\n",
    "There are multiple ways you can download a GitHub repo.\n",
    "\n",
    "First, you can clone it using git, then we process each file and prepare it for ingestion into our search system.\n",
    "\n",
    "Alternatively, we can download the entire repository as a zip file and process all the content.\n",
    "\n",
    "# Working with Zip Archives\n",
    "\n",
    "The second option is easier and more efficient for our use case.\n",
    "\n",
    "We don't even need to save the zip archive \\- we can load it into our Python process memory and extract all the data we need from there.\n",
    "\n",
    "So the plan:\n",
    "\n",
    "- Use `requests` for downloading the zip archive from GitHub  \n",
    "- Open the archive using built-in `zipfile` and `io` modules  \n",
    "- Iterate over all `.md` and `.mdx` files in the repo  \n",
    "- Collect the results into a list\n",
    "\n",
    "Let's implement it step by step.\n",
    "\n",
    "First, we import the necessary libraries:\n",
    "\n",
    "```py\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "```\n",
    "\n",
    "Next, we download the repository as a zip file. GitHub provides a convenient URL format for this:\n",
    "\n",
    "```py\n",
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n",
    "```\n",
    "\n",
    "Now we process the zip file in memory without saving it to disk:\n",
    "\n",
    "```py\n",
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n",
    "```\n",
    "\n",
    "Let's look at what we got:\n",
    "\n",
    "```py\n",
    "print(repository_data[1])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```py\n",
    "{'id': '9e508f2212',\n",
    " 'question': 'Course: When does the course start?',\n",
    " 'sort_order': 1,\n",
    " 'content': \"...'}\n",
    "```\n",
    "\n",
    "For processing Evidently docs we also need `.mdx` files (React markdown), so we can modify the code like this:\n",
    "\n",
    "```py\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "\n",
    "    # rest remains the same...\n",
    "```\n",
    "\n",
    "## \n",
    "\n",
    "# Complete Implementation\n",
    "\n",
    "Let's now put everything together into a reusable function:\n",
    "\n",
    "```py\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n",
    "```\n",
    "\n",
    "We can now use this function for different repositories:\n",
    "\n",
    "```py\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "```\n",
    "\n",
    "## \n",
    "\n",
    "# Data Processing Considerations\n",
    "\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" \\- breaking large documents into smaller, manageable pieces. This is important because:\n",
    "\n",
    "1. Search relevance: Smaller chunks are more specific and relevant to user queries  \n",
    "2. Performance: AI models work better with shorter text segments  \n",
    "3. Memory limits: Large documents might exceed token limits of language models\n",
    "\n",
    "We will cover chunking techniques in tomorrow's lesson.\n",
    "\n",
    "If you have any suggestions about the course content or want to improve something, let me know\\!\n",
    "\n",
    "# Homework\n",
    "\n",
    "- Create a new `uv` project in the `project` directory  \n",
    "- Select a GitHub repo with documentation (preferably with `.md` files)  \n",
    "- Download the data from there using the techniques we've learned  \n",
    "- Make a post on social media about what you're building\n",
    "\n",
    "# Learning in Public\n",
    "\n",
    "We encourage everyone to share what they learned. This is called \"learning in public\".\n",
    "\n",
    "Learning in public is one of the most effective ways to accelerate your growth. Here's why:\n",
    "\n",
    "1. Accountability: Sharing your progress creates commitment and motivation to continue  \n",
    "2. Feedback: The community can provide valuable suggestions and corrections  \n",
    "3. Networking: You'll connect with like-minded people and potential collaborators  \n",
    "4. Documentation: Your posts become a learning journal you can reference later  \n",
    "5. Opportunities: Employers and clients often discover talent through public learning\n",
    "\n",
    "Don't worry about being perfect. Everyone starts somewhere, and people love following genuine learning journeys\\!\n",
    "\n",
    "## Example post for LinkedIn\n",
    "\n",
    "| üöÄ Day 1 of AI Agents crash course by @Alexey Grigorev complete\\! Just built my first data ingestion pipeline that can download and parse any GitHub repository's documentation. Today I learned how to: ‚úÖ Download repos as zip archives ‚úÖ Parse frontmatter metadata ‚úÖ Extract content from markdown files Here's my repo:  Next up: Chunking the data for better search performance\\! Following along with this amazing course \\- who else is building AI agents? You can sign up here: [https://alexeygrigorev.com/aihero/](https://alexeygrigorev.com/aihero/) |\n",
    "| :---- |\n",
    "\n",
    "## Example post for Twitter/X\n",
    "\n",
    "| ü§ñ Built my first AI data pipeline in a course from @Al\\_Grigor ‚ú® Downloads any GitHub repo üìù Parses markdown üîç Prepares data for AI search Here's my repo:  Tomorrow: Chunking the data for search performance\\! Join me too: [https://alexeygrigorev.com/aihero/](https://alexeygrigorev.com/aihero/) |\n",
    "| :---- |\n",
    "\n",
    "# Community\n",
    "\n",
    "Have questions about this lesson or suggestions for improvement? You can find me and other learners in **DataTalks.Club Slack**:\n",
    "\n",
    "- [Join DataTalks.Club](https://datatalks.club/slack.html)  \n",
    "- Find us in the [`#course-ai-bootcamp` channel](https://app.slack.com/client/T01ATQK62F8/C09DLTMKVHV)\n",
    "\n",
    "In the community channel you can:\n",
    "\n",
    "- Ask questions about the course content  \n",
    "- Share your implementation and get feedback  \n",
    "- Show off your GitHub repositories  \n",
    "- Suggest improvements to the course materials  \n",
    "- Connect with other course participants\n",
    "\n",
    "Don't hesitate to reach out \\- the community is here to help each other succeed\\!\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
